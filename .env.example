# =============================================================================
# CONFIGURACIÓN DEL ASISTENTE ORGANIZACIONAL - MODELO LOCAL
# =============================================================================
# Copia este archivo a .env y ajusta según necesites

# -----------------------------------------------------------------------------
# Ollama (Modelo LLM Local)
# -----------------------------------------------------------------------------
# URL del servidor Ollama (por defecto corre en localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Modelo LLM a usar (debe estar descargado con 'ollama pull')
# Recomendado: llama3.1:8b-instruct-q4_K_M (balance calidad/velocidad, ~5GB)
# Alternativas:
#   - llama3.2:3b (más rápido, menor calidad, ~2GB)
#   - mistral:7b (bueno en razonamiento, ~4GB)
#   - qwen2.5:7b (excelente multilingüe, ~5GB)
OLLAMA_MODEL=llama3.1:8b-instruct-q4_K_M

# -----------------------------------------------------------------------------
# Modelo de Embeddings (Local - Sentence Transformers)
# -----------------------------------------------------------------------------
# Modelo recomendado para español/multilingüe:
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Alternativas (requieren más RAM pero mejor calidad):
# - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (mejor español)
# - BAAI/bge-base-en-v1.5 (mejor calidad general, más lento)

# -----------------------------------------------------------------------------
# Rutas de datos
# -----------------------------------------------------------------------------
VECTOR_STORE_PATH=data/embeddings/chroma
KNOWLEDGE_BASE_PATH=data/processed
RAW_DATA_PATH=data/raw

# -----------------------------------------------------------------------------
# Configuración del servidor
# -----------------------------------------------------------------------------
SERVICE_HOST=0.0.0.0
SERVICE_PORT=8000
STREAMLIT_PORT=8501

# -----------------------------------------------------------------------------
# CORS y API URLs
# -----------------------------------------------------------------------------
ALLOWED_ORIGINS=http://localhost:8501,http://127.0.0.1:8501
API_BASE_URL=http://localhost:8000

# =============================================================================
# AUTENTICACIÓN Y SEGURIDAD (RS1, RS2)
# =============================================================================
# ⚠️ IMPORTANTE: Cambiar SECRET_KEY en producción a una cadena aleatoria de 32+ caracteres
# Generar con: python -c "import secrets; print(secrets.token_urlsafe(32))"
SECRET_KEY=dev-secret-key-change-in-production

# Tiempo de expiración del access token (en minutos)
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Tiempo de expiración del refresh token (en minutos, default: 7 días = 10080)
REFRESH_TOKEN_EXPIRE_MINUTES=10080

# Ruta de la BD de autenticación y usuarios
AUTH_DB_PATH=data/auth/auth.db

# =============================================================================
# CIFRADO EN REPOSO (RS4)
# =============================================================================
# ⚠️ CRÍTICO EN PRODUCCIÓN: Generar llave con:
#    python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
# Guardar en variable segura (secretos de sistema, vault, etc.)
# NO compartir ni versionar en control de código
ENCRYPTION_KEY=

# Activar cifrado automático en bases de datos
ENABLE_DB_ENCRYPTION=true

# Directorio para almacenar claves de cifrado (permisos 0700)
ENCRYPTED_KEYS_PATH=data/encrypted_keys

# =============================================================================
# HTTPS/SSL (RS4 - Producción)
# =============================================================================
# En desarrollo: usar http (falso) y nginx/reverse proxy en producción
SSL_ENABLED=false

# Rutas a certificado SSL (para desarrollo, opcional)
# En producción, usar nginx con certificado Let's Encrypt
# SSL_CERTFILE=certs/cert.pem
# SSL_KEYFILE=certs/key.pem
