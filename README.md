# ğŸ¤– Asistente Organizacional - MVP

Prototipo basado en **RAG (Retrieval Augmented Generation)** para apoyar la capacitaciÃ³n e inducciÃ³n de colaboradores, brindando acceso inteligente a procedimientos crÃ­ticos, polÃ­ticas y resoluciones histÃ³ricas.

**Stack tecnolÃ³gico:**
- ğŸ§  LLM: Ollama (llama3.1:8b-instruct local)
- ğŸ“š Vector Store: ChromaDB
- ğŸ” Embeddings: Sentence Transformers
- âš¡ API: FastAPI
- ğŸ¨ UI: Streamlit
- ğŸ“Š Analytics: SQLite + Plotly
- ğŸ’¬ Memoria Conversacional: SQLite con gestiÃ³n de sesiones
- ğŸ Python 3.10+

---

## ğŸ“ Estructura del Proyecto

```
org-assistant/
â”œâ”€â”€ config/              # ConfiguraciÃ³n (settings.py, .env)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # ğŸ“‚ Documentos originales (.txt, .pdf, .docx, etc.)
â”‚   â”œâ”€â”€ processed/      # ğŸ“ Chunks procesados (generado automÃ¡ticamente)
â”‚   â”œâ”€â”€ embeddings/     # ğŸ—„ï¸ Vector store ChromaDB (generado automÃ¡ticamente)
â”‚   â”œâ”€â”€ analytics/      # ğŸ“Š Base de datos de mÃ©tricas (generado automÃ¡ticamente)
â”‚   â”œâ”€â”€ sessions/       # ğŸ’¬ Sesiones conversacionales (generado automÃ¡ticamente)
â”‚   â””â”€â”€ feedback/       # ğŸ“ Feedback de usuarios (generado automÃ¡ticamente)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ knowledge_base/ # Pipeline de ingesta y procesamiento
â”‚   â”œâ”€â”€ rag_engine/     # LÃ³gica RAG (retrieval + generaciÃ³n)
â”‚   â”œâ”€â”€ service/        # API FastAPI + admin routes
â”‚   â”œâ”€â”€ ui/             # Interfaz Streamlit (app.py + chat_app.py + admin_dashboard.py)
â”‚   â”œâ”€â”€ analytics/      # Sistema de tracking y mÃ©tricas
â”‚   â”œâ”€â”€ memory/         # GestiÃ³n de memoria conversacional
â”‚   â”œâ”€â”€ predictive/     # Motor predictivo y recomendaciones
â”‚   â”œâ”€â”€ admin/          # GestiÃ³n de documentos y feedback
â”‚   â””â”€â”€ evaluation/     # MÃ©tricas y evaluaciÃ³n offline
â”œâ”€â”€ reingest.py                 # Script de reingesta completa
â”œâ”€â”€ reset_knowledge.py          # Script para limpiar vector store
â”œâ”€â”€ run_analytics_dashboard.py  # Script para lanzar dashboard de analytics
â”œâ”€â”€ run_admin_dashboard.py      # Script para lanzar panel de administraciÃ³n
â”œâ”€â”€ test_api.py                 # DiagnÃ³stico del sistema
â”œâ”€â”€ REINGESTA.md                # GuÃ­a detallada de reingesta
â”œâ”€â”€ ANALYTICS.md                # DocumentaciÃ³n del sistema de mÃ©tricas
â”œâ”€â”€ MEMORIA_CONVERSACIONAL.md   # DocumentaciÃ³n de memoria conversacional
â”œâ”€â”€ SISTEMA_PREDICTIVO.md       # DocumentaciÃ³n del motor predictivo
â”œâ”€â”€ ADMIN.md                    # DocumentaciÃ³n del sistema de administraciÃ³n
â””â”€â”€ MANUAL_USO_RAPIDO.md        # ğŸ“– Manual rÃ¡pido para usuarios y administradores
```

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1ï¸âƒ£ Prerrequisitos

**Software:**
- Python 3.10 o superior
- Git
- **Ollama instalado** (para el modelo LLM local)
  - Descarga desde: https://ollama.com
  - Compatible con Windows, macOS y Linux
  - Instala y asegÃºrate de que estÃ© corriendo
- ConexiÃ³n a internet (solo para la instalaciÃ³n inicial y descarga de modelos)

**Hardware recomendado:**
- **MÃ­nimo:** 8GB RAM, CPU moderna (puede funcionar solo con CPU)
- **Recomendado:** 16GB+ RAM, GPU con 6GB+ VRAM (NVIDIA/AMD/Apple Silicon)
- **Ã“ptimo:** 32GB RAM, GPU con 8GB+ VRAM

**Nota sobre modelos:**
- `llama3.1:8b-instruct-q4_K_M` (recomendado): ~5GB de RAM/VRAM
- `llama3.2:3b` (alternativa ligera): ~2GB de RAM/VRAM
- Sin GPU: El modelo funcionarÃ¡ en CPU, serÃ¡ mÃ¡s lento pero funcional

### 2ï¸âƒ£ Clonar el repositorio

```bash
git clone <URL_DEL_REPOSITORIO>
cd org-assistant
```

### 3ï¸âƒ£ Crear entorno virtual

**Windows:**
```bash
python -m venv .venv
.venv\Scripts\activate
```

**Linux/Mac:**
```bash
python -m venv .venv
source .venv/bin/activate
```

### 4ï¸âƒ£ Instalar dependencias

```bash
pip install -e .
```

Para desarrollo (incluye pytest, ruff):
```bash
pip install -e ".[dev]"
```

### 5ï¸âƒ£ Configurar variables de entorno

```bash
# Windows
copy .env.example .env

# Linux/Mac
cp .env.example .env
```

**âš ï¸ IMPORTANTE:** Antes de continuar, asegÃºrate de tener Ollama corriendo y descarga el modelo:

```bash
# Verifica que Ollama estÃ© instalado
ollama --version

# Descarga el modelo recomendado (Llama 3.1 8B cuantizado)
ollama pull llama3.1:8b-instruct-q4_K_M

# Verifica que el modelo estÃ© disponible
ollama list
```

El archivo `.env` ya tiene la configuraciÃ³n correcta por defecto:
- `OLLAMA_BASE_URL=http://localhost:11434` (puerto por defecto de Ollama)
- `OLLAMA_MODEL=llama3.1:8b-instruct-q4_K_M` (modelo recomendado)

### 6ï¸âƒ£ Agregar documentos

Coloca tus documentos organizacionales en `data/raw/`:

```bash
data/raw/
â”œâ”€â”€ politicas/
â”‚   â””â”€â”€ trabajo_remoto.pdf
â”œâ”€â”€ procedimientos/
â”‚   â””â”€â”€ reembolsos.md
â””â”€â”€ ejemplo_documento.txt  # â† Ya incluido como ejemplo
```

**Formatos soportados:** `.txt`, `.md`, `.pdf`, `.docx`, `.csv`, `.json`

> ğŸ’¡ **Tip:** Organiza los documentos en subcarpetas por proceso/departamento para facilitar filtros de bÃºsqueda.

### 7ï¸âƒ£ Ejecutar ingesta inicial

```bash
python reingest.py
```

Este script:
- Procesa todos los documentos de `data/raw/`
- Extrae keywords con TF-IDF
- Genera embeddings
- Construye el vector store

â±ï¸ **Tiempo estimado:** 2-5 minutos (depende de cantidad de documentos)

### 8ï¸âƒ£ Verificar instalaciÃ³n

```bash
python test_api.py
```

Si todos los tests pasan âœ…, estÃ¡s listo para usar el sistema.

### 9ï¸âƒ£ Verificar que hay documentos en el vector store

```bash
python check_docs.py
```

Este script muestra cuÃ¡ntos documentos hay en tu base de conocimientos. Si sale 0, ejecuta `python reingest.py`.

---

## ğŸ”’ ConfirmaciÃ³n de Modelo Local (100% Privado)

Tu asistente funciona **completamente en local** sin enviar datos a internet:

### âœ… CÃ³mo verificarlo:

**1. Revisa la configuraciÃ³n actual:**
```bash
python -c "from config.settings import get_settings; s = get_settings(); print(f'Modelo: {s.ollama_model}'); print(f'URL: {s.ollama_base_url}')"
```

DeberÃ­as ver:
```
Modelo: llama3.1:8b-instruct-q4_K_M
URL: http://localhost:11434
```

**2. Monitorea el uso de recursos:**
- Abre el monitor de sistema de tu SO (Administrador de Tareas en Windows, Activity Monitor en Mac, htop en Linux)
- Observa el uso de CPU/GPU mientras haces una consulta
- VerÃ¡s un pico de uso porque el modelo se ejecuta localmente en tu mÃ¡quina

**3. Prueba sin internet:**
- Desconecta tu WiFi
- Haz una consulta
- FuncionarÃ¡ perfectamente porque todo es local

**GarantÃ­as de privacidad:**
- âœ… NingÃºn dato sale de tu mÃ¡quina
- âœ… No hay API keys de servicios externos
- âœ… Puedes apagar Ollama cuando no lo uses
- âœ… El modelo se ejecuta 100% en tu hardware local (CPU/GPU)

---

## ğŸ¯ Uso del Sistema

> ğŸ“– **Â¿Primera vez usando el sistema?** Lee el [**MANUAL DE USO RÃPIDO**](MANUAL_USO_RAPIDO.md) - GuÃ­a paso a paso para usuarios y administradores.

### Iniciar los servicios

**Terminal 1 - Servidor API:**
```bash
uvicorn src.service.app:app --reload
```

**Terminal 2 - Interfaz Streamlit (BÃ¡sica):**
```bash
streamlit run src/ui/app.py
```

**O Terminal 2 - Interfaz de Chat (Con memoria conversacional):**
```bash
python run_chat.py
```

**Terminal 3 (Opcional) - Dashboard de Analytics:**
```bash
python run_analytics_dashboard.py
```

**Terminal 4 (Opcional) - Panel de AdministraciÃ³n:**
```bash
python run_admin_dashboard.py
```

### Acceder a la interfaz

Abre tu navegador en:
- **UI BÃ¡sica:** http://localhost:8501
- **Chat con Memoria:** http://localhost:8503
- **Analytics Dashboard:** http://localhost:8502
- **Admin Dashboard:** http://localhost:8504
- **API Docs:** http://localhost:8000/docs
- **API Analytics:** http://localhost:8000/analytics?days=7
- **API Predictivo:** http://localhost:8000/predictive/insights?days=7
- **API Recomendaciones:** http://localhost:8000/predictive/recommendations?question=vacaciones
- **API Alertas:** http://localhost:8000/predictive/alerts
- **API Admin Docs:** http://localhost:8000/admin/documents
- **Health Check:** http://localhost:8000/health

### Hacer preguntas

Desde la interfaz Streamlit:
1. Escribe tu pregunta (ej: "Â¿CuÃ¡l es el proceso para solicitar vacaciones?")
2. (Opcional) Agrega filtros por proceso o responsable
3. Haz clic en "Consultar"
4. Revisa la respuesta y las referencias a documentos
5. Proporciona feedback (Ãºtil/no Ãºtil) para mejorar el sistema

### Usar el Chat con Memoria Conversacional

La nueva interfaz de chat (http://localhost:8503) permite:
- **Conversaciones naturales:** El asistente recuerda el contexto
- **Preguntas de seguimiento:** "Â¿Y si...?", "Dame mÃ¡s detalles sobre eso"
- **Historial persistente:** Mantiene la conversaciÃ³n activa durante 24 horas
- **Referencias inline:** Ve las fuentes sin salir del chat
- **Limpiar chat:** Reinicia la conversaciÃ³n cuando quieras

Ejemplo de conversaciÃ³n:
```
Usuario: Â¿CuÃ¡l es el proceso de solicitud de vacaciones?
Asistente: [Responde con el proceso completo]

Usuario: Â¿Y si necesito mÃ¡s de 15 dÃ­as?
Asistente: [Responde considerando el contexto anterior]

Usuario: Â¿QuiÃ©n debe aprobar la solicitud?
Asistente: [Responde con informaciÃ³n especÃ­fica del proceso]
```

### Monitorear mÃ©tricas de uso

Desde el Dashboard de Analytics (http://localhost:8502):
- **KPIs principales:** Consultas totales, satisfacciÃ³n, cobertura, tiempo de respuesta
- **Volumen:** Tendencia de consultas por dÃ­a
- **SatisfacciÃ³n:** Tendencia y gauge de satisfacciÃ³n del usuario
- **Top consultas:** Preguntas mÃ¡s frecuentes
- **Top temas:** Procesos mÃ¡s consultados
- **Cobertura:** DistribuciÃ³n de consultas exitosas vs fallidas
- **Impacto organizacional:** Tiempo ahorrado, eficiencia del sistema
- **Recomendaciones:** Sugerencias automÃ¡ticas de mejora

### Gestionar Ollama (liberar recursos cuando no lo uses)

Ollama consume recursos solo durante las consultas. Para liberar completamente la memoria:

**Windows:**
```bash
# Cerrar desde el Administrador de Tareas o:
taskkill /IM ollama.exe /F

# Para reiniciar: busca "Ollama" en el menÃº inicio
```

**macOS:**
```bash
# Detener:
brew services stop ollama

# Iniciar:
brew services start ollama
```

**Linux:**
```bash
# Detener:
sudo systemctl stop ollama

# Iniciar:
sudo systemctl start ollama
```

---

## âš™ï¸ Panel de AdministraciÃ³n

### GestiÃ³n de Documentos sin CÃ³digo

El sistema incluye un **Panel de AdministraciÃ³n** completo para gestionar documentos sin necesidad de conocimientos tÃ©cnicos:

**Acceso:**
```bash
python run_admin_dashboard.py
# Abre en: http://localhost:8504
```

**Funcionalidades:**

1. **ğŸ“ GestiÃ³n de Documentos:**
   - Subir documentos (.txt, .md, .pdf, .docx, .doc)
   - Listar todos los documentos indexados
   - Eliminar documentos obsoletos
   - Ver estadÃ­sticas del sistema
   - Re-ingesta automÃ¡tica tras cambios

2. **ğŸ’¬ GestiÃ³n de Feedback:**
   - Revisar feedback negativo
   - Categorizar problemas (info faltante, respuesta incorrecta, poco clara)
   - Marcar como accionado con notas
   - Ver top temas con problemas

3. **ğŸ“Š EstadÃ­sticas:**
   - Tasa de satisfacciÃ³n del usuario
   - Total de feedback positivo/negativo
   - Feedback pendiente de revisiÃ³n
   - DistribuciÃ³n por categorÃ­a

**Ventajas del Panel de Admin:**
- âœ… Sin necesidad de editar cÃ³digo o reiniciar servidor
- âœ… Cambios reflejados inmediatamente
- âœ… DetecciÃ³n automÃ¡tica de duplicados
- âœ… Loop de mejora continua con feedback
- âœ… Interface intuitiva para usuarios no tÃ©cnicos

ğŸ“– **DocumentaciÃ³n completa:** Ver [ADMIN.md](ADMIN.md)

**âš ï¸ Nota sobre documentos existentes:**
Si tienes documentos en `data/raw/` que fueron agregados **antes** de usar el panel de admin, necesitas registrarlos:
```bash
python register_existing_docs.py
```
Esto solo es necesario una vez. Los documentos subidos mediante el panel se registran automÃ¡ticamente.

---

## ğŸ”„ Actualizar Documentos (MÃ©todo Manual)

### Reingesta completa (desde cero)

**Nota:** Este mÃ©todo es para usuarios tÃ©cnicos. **Se recomienda usar el Panel de AdministraciÃ³n** (ver secciÃ³n anterior).

```bash
# 1. Resetear base de conocimientos
python reset_knowledge.py

# 2. Actualizar/agregar documentos en data/raw/

# 3. Reingestar todo
python reingest.py

# 4. Reiniciar servidor FastAPI (Ctrl+C y volver a ejecutar)
```

ğŸ“– **MÃ¡s detalles:** Ver [REINGESTA.md](REINGESTA.md)

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se puede conectar con Ollama"

**SoluciÃ³n:** AsegÃºrate de que Ollama estÃ© corriendo y el modelo estÃ© descargado.

```bash
# Verificar que Ollama estÃ© corriendo
ollama list

# Si no estÃ¡ instalado, descarga desde: https://ollama.com

# Descargar el modelo si no estÃ¡ disponible
ollama pull llama3.1:8b-instruct-q4_K_M
```

### Error: "El vector store no se ha inicializado"

**SoluciÃ³n:** Ejecuta la ingesta:

```bash
python reingest.py
```

### Error: "Internal Server Error" en Streamlit

**SoluciÃ³n:** Revisa los logs del servidor FastAPI. Causas comunes:
- El vector store no existe â†’ Ejecuta `python reingest.py`
- Ollama no estÃ¡ corriendo â†’ Verifica con `ollama list`
- El modelo no estÃ¡ descargado â†’ Ejecuta `ollama pull llama3.1:8b-instruct-q4_K_M`
- Falta alguna dependencia â†’ Ejecuta `pip install -e .`

### Los documentos no se reflejan en las respuestas

**SoluciÃ³n:** Debes reingestar Y reiniciar el servidor:

```bash
python reingest.py
# Luego Ctrl+C en el servidor y volver a ejecutar:
uvicorn src.service.app:app --reload
```

### DiagnÃ³stico completo del sistema

```bash
python test_api.py
```

Este script verifica:
- âœ… ConfiguraciÃ³n cargada correctamente
- âœ… Vector store funcional
- âœ… ConexiÃ³n con Ollama (modelo local)
- âœ… Pipeline completo end-to-end

---

## ğŸ“š Scripts Ãštiles

| Script | DescripciÃ³n |
|--------|-------------|
| `python reingest.py` | Reingesta completa de documentos |
| `python reset_knowledge.py` | Limpia vector store y procesados |
| `python test_api.py` | DiagnÃ³stico completo del sistema |
| `python run_chat.py` | Lanzar chat con memoria conversacional |
| `python run_analytics_dashboard.py` | Lanzar dashboard de analytics |
| `uvicorn src.service.app:app --reload` | Iniciar servidor API |
| `streamlit run src/ui/app.py` | Iniciar interfaz web bÃ¡sica |

---

## ğŸ¤ ContribuciÃ³n al Proyecto

### Para el equipo de desarrollo

1. **Crea una rama para tu feature:**
   ```bash
   git checkout -b feature/nombre-descriptivo
   ```

2. **Instala dependencias de desarrollo:**
   ```bash
   pip install -e ".[dev]"
   ```

3. **Ejecuta los tests antes de hacer commit:**
   ```bash
   pytest
   ruff check .
   ```

4. **Haz commit y push:**
   ```bash
   git add .
   git commit -m "DescripciÃ³n clara del cambio"
   git push origin feature/nombre-descriptivo
   ```

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Cambiar el tamaÃ±o de chunks

Edita `src/knowledge_base/ingest.py`:

```python
DEFAULT_SPLITTER = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Caracteres por chunk
    chunk_overlap=150,    # Solapamiento entre chunks
)
```

### Cambiar el modelo de embeddings

Edita `.env`:

```bash
# Para mejor soporte multilingÃ¼e/espaÃ±ol:
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
```

**âš ï¸ Importante:** Si cambias el modelo de embeddings, debes hacer una reingesta completa.

### Cambiar el modelo LLM

Primero descarga el modelo con Ollama:

```bash
# Modelo mÃ¡s pequeÃ±o y rÃ¡pido (3B):
ollama pull llama3.2:3b

# Modelo equilibrado (recomendado, 8B):
ollama pull llama3.1:8b-instruct-q4_K_M

# Modelo mÃ¡s grande y preciso (7B):
ollama pull mistral:7b
```

Luego edita `.env`:

```bash
# Ejemplo para cambiar al modelo mÃ¡s pequeÃ±o:
OLLAMA_MODEL=llama3.2:3b

# O al modelo Mistral:
OLLAMA_MODEL=mistral:7b
```

---

## ğŸ“ Notas Importantes

- **Seguridad:** Nunca subas el archivo `.env` al repositorio (estÃ¡ en `.gitignore`)
- **Datos:** Los documentos en `data/raw/` NO se suben al repo por defecto (solo ejemplos)
- **Modelo Local:** El modelo LLM se ejecuta localmente en tu mÃ¡quina. AsegÃºrate de tener Ollama corriendo
- **Recursos:** El modelo 8B cuantizado usa ~5GB de VRAM/RAM. Puedes usar modelos mÃ¡s pequeÃ±os (3B) si tienes recursos limitados
- **Performance:** El primer arranque descarga modelos (~90MB embeddings + ~5GB LLM), es normal que tome tiempo
- **Feedback:** El sistema guarda feedback en `data/feedback/feedback.jsonl` para anÃ¡lisis

---

## ğŸ”— Enlaces Ãštiles

- [Descargar Ollama](https://ollama.com)
- [Modelos disponibles en Ollama](https://ollama.com/library)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Streamlit Documentation](https://docs.streamlit.io/)

---

## ğŸ‘¥ Equipo

**UDLA Prototype Team**

Para preguntas o soporte, contacta al equipo de desarrollo.

---

## ğŸ“„ Licencia

MIT License - Ver archivo LICENSE para mÃ¡s detalles.

---

Â¿Preguntas? Revisa [REINGESTA.md](REINGESTA.md) para mÃ¡s detalles sobre el manejo de documentos.
